logical_vec = vector(length = length(list_of_dfs))
# check it for each dataframe
for (df in seq_along(list_of_dfs)) {
colnames_df = names(list_of_dfs[[df]])
if (!"subid" %in% colnames_df || !"id" %in% colnames_df) {
print(paste0(names(list_of_dfs)[[df]], ": does not have the id and subid col"))
logical_vec[[df]] = FALSE
} else if (length(list_of_dfs[[df]][["id"]]) < 1 ||
length(list_of_dfs[[df]][["subid"]]) < 1) {
print(paste0(names(list_of_dfs)[[df]], ": subid or id colum have length 0"))
logical_vec[[df]] = FALSE
}
else{
logical_vec[[df]] = TRUE
}
}
# return the vecor for indexing later
return(logical_vec)
}
#' create the index to join it later to the shapefile
create_iffi_index = function(list_of_dfs, log_idx) {
for (df in seq_along(list_of_dfs)) {
# if the df has those two columns
if (log_idx[[df]]) {
id = list_of_dfs[[df]][["id"]] * 10000
subid = list_of_dfs[[df]][["subid"]] * 100
iffi_kodex = id + subid + 00
# # create iffi_kodex
# iffi_kodex = paste0(id_p_subid)
# add it to the df
list_of_dfs[[df]][["iffi_kodex"]] = iffi_kodex
print(paste0("Created iffi-kodex for: ", names(list_of_dfs)[[df]]))
}
}
return(list_of_dfs)
}
#' find the necessary tables
find_tables = function(list_of_dfs_with_iffi_kodex, attributes){ # great name...
# make a list of all tables-names (names) and the columns (keys)
tables_names_columns = lapply(list_of_dfs_with_iffi_kodex, names)
# now find the tables you need
tables = vector()
# counter to count up the tables vector
j = 1
# for each attribute
for (attr in attributes) {
# for each tables
for (i in seq_along(tables_names_columns)) {
# if the attribute is in one of the colums of the table
if (attr %in% tables_names_columns[[i]] ) {
print(paste0("The variable: ", attr, " is found in table: ", names(tables_names_columns)[[i]]))
# it its not yet in the list of tables
if (! names(tables_names_columns)[[i]] %in% tables) {
tables[[j]] = names(tables_names_columns)[[i]]
j = j+1
}
}
}
}
return(tables)
}
#' joins a sinle sf-object with a column "PIFF_ID" to another table
#' with a column "iffi_kodex"
join_on_iffikodex = function(shape, table2){
if (! "NUMEROGIS" %in% names(shape)) {
stop(call. = FALSE, "The does not have a column named NUMEROGIS")
}
merged_sf = merge(shape, table2, by.x="PIFF_ID", by.y="iffi_kodex", all.x = T, all.y=FALSE)
return(merged_sf)
}
#' return a specific dataframe from a named list of dataframes
return_df_on_table_name = function(list_of_dfs, table_name) {
idx = which(names(list_of_dfs) == table_name)
return(list_of_dfs[[idx]])
}
join_shape_attributes = function(shape, names_of_tables, dfs_attr_iffi){
merged_shape = shape
for (table in names_of_tables) {
# get the attribute table
single_df_attr = return_df_on_table_name(dfs_attr_iffi, table)
# update the shape
merged_shape = join_on_iffikodex(merged_shape, single_df_attr)
}
return(merged_shape)
}
# small helper
split_and_return = function(x){
cols = vector()
i = 1
for (table in x) {
col = str_split(table, "\\.") %>% .[[1]] %>% .[[3]]
cols[[i]] = col
i = i +1
}
return(cols)
}
join_descriptions = function(joins, dfs_attr_iffi, dfs_dict){
# list of joined tables
joined_tables = vector("list")
for (i in seq_along(joins)) {
#------------
# get the database, Table, column of the attribute datbase
table_attri = names(joins)[[i]] %>% stringr::str_split(., pattern = "\\.") %>% .[[1]] %>% .[[2]]
col_attri = names(joins)[[i]] %>% stringr::str_split(., pattern = "\\.") %>% .[[1]] %>% .[[3]]
# find the attribute table
df_index = grep(pattern = paste0("^", table_attri, "$"), names(dfs_attr_iffi))
# select the column to join and the iffi kodex
df = dfs_attr_iffi[[df_index]] %>% select(c(col_attri, "iffi_kodex"))
#------------
# get the database, Table, column of the attribute datbase
table_dict = joins[[i]] %>% stringr::str_split(., pattern = "\\.") %>% .[[1]] %>% .[[2]]
cols_dict = sapply(joins[[i]], function(x) split_and_return(x), USE.NAMES = F)
# find the dict table
df_dict_index = grep(table_dict, names(dfs_dict))
df_dict = dfs_dict[[df_dict_index]] %>% select(cols_dict)
#--------------------
# join them
merged = merge(df, df_dict, by.x = col_attri, by.y = cols_dict[[1]], all.x = T, all.y=F)
print(paste0("joind the tables: ", table_attri, " and ", table_dict))
print(paste0("    on the columns ", col_attri, " and ", cols_dict[[1]]))
print(paste0("    Resulting table is has ", dim(merged)[[1]], " rows, and ", dim(merged)[[2]], " columns"))
print("")
joined_tables[[i]] = merged
}
return(joined_tables)
}
#' join each table in a list of tables from the dict on the shape
join_descriptions_shape = function(list_of_dfs_dict, shape) {
final_shape = shape
# test_list = vector("list"); i = 1
for(table_dict in list_of_dfs_dict){
# test_list[[i]] = final_shape
print(nrow(final_shape))
print(nrow(table_dict))
final_shape = join_on_iffikodex(final_shape, table_dict)
# i = i+1
}
return(final_shape)
}
select_cols = function(df, attri, joins){
# the directly selected columns
column_names = attri
# the columns we want from the dictionary
cols_dict = vector(length = length(joins))
for (i in seq_along(joins)) {
col = sapply(joins[[i]], function(x) split_and_return(x), USE.NAMES = F) %>% .[[2]]
cols_dict[[i]] = col
}
final_cols = c("PIFF_ID", column_names, cols_dict)
# select them
df = df %>% select(final_cols)
return(df)
}
i = 1
col = sapply(joins[[i]], function(x) split_and_return(x), USE.NAMES = F) %>% .[[2]]
library(stringr)
col = sapply(joins[[i]], function(x) split_and_return(x), USE.NAMES = F) %>% .[[2]]
col
col = sapply(joins[[i]], function(x) split_and_return(x), USE.NAMES = T) %>% .[[2]]
col
col = sapply(joins[[i]], function(x) split_and_return(x), USE.NAMES = F) %>% .[[2]]
col = sapply(joins[[i]], function(x) split_and_return(x), USE.NAMES = F)
col
# filter all samples that have at least one for each, so we have a full date
res_filtered = res_raw %>%
dplyr::filter(
!is.na(anno_min) | !is.na(anno_max),
!is.na(mese_min) | !is.na(mese_max),
!is.na(giorno_min) | !is.na(giorno_max)
) %>%
distinct(PIFF_ID, .keep_all = TRUE)
res_no_date = res_raw %>% filter(!PIFF_ID %in% res_filtered$PIFF_ID) %>%
distinct(PIFF_ID, .keep_all = TRUE)
# build a date
res_date = res_filtered %>% mutate(
year = if_else(!is.na(anno_min), true = anno_min, false = anno_max),
month = if_else(!is.na(mese_min), true = mese_min, false = mese_max),
day = if_else(!is.na(giorno_min), true = giorno_min, false = giorno_max)
) %>% mutate(date = paste(year, month, day, sep = "/")) %>%
mutate(date = as.Date(date),
year.int = year,
year.posix = as.Date(paste0(year, "-01-01")))
res_date
# check that there are no more NAs
res_date %>%  select(c(year.posix, month, day)) %>% st_drop_geometry(.) %>% purrr::map(~sum(is.na(.)))
# 1900 - now
ggplot(res_date) +
geom_histogram(aes(x = year.posix), fill = "darkred", col = "black") +
scale_x_date(date_breaks = "10 year",
labels = scales::date_format("%Y"),
limits = as.Date(c("1900-01-01", "2020-12-01")))
# 2000 -now
ggplot(res_date, aes(x = year.posix)) +
geom_bar(fill = "grey", col = "black", size=1.2) +
geom_text(aes(label=..count..),
stat='count',
nudge_y = 0.125,
vjust=-1) +
scale_x_date(date_breaks = "1 year",
labels = scales::date_format("%Y"),
limits = as.Date(c("2000-01-02", "2020-12-01"))) +
labs(title="Count of Events of all type",
x = "Year") +
theme_minimal() +
theme(
axis.text.x = element_text(angle=90)
)
# plot it
mv = res_date %>% filter(year > 1900) %>% mapview(zcol = "year.int", at = seq(1900, 2020, 20))
mv
library(iffitoR)
test = make_shapefile(
database_dir = "../euracR/data/database/",
# normally null only setting it here for me
attribute_database_name = "tbl_frane",
# the name without extension
dictionary_database_name = "diz_frane",
shapefile = "../euracR/data/Shapefiles/IFFI10_1.shp",
# normally null only setting it here for me
# the colums we want to retrieve directly
attri = c(
"anno_min",
"anno_max",
"mese_min",
"mese_max",
"giorno_min",
"giorno_max"
),
# tables to join the description
joins = list(join = "tbl_frane.clas_ii_liv.acqua" = c("diz_frane.diz_acqua.acqua",
"diz_frane.diz_acqua.nome_acqua"))
)
test = make_shapefile(
database_dir = "../euracR/data/database/",
# normally null only setting it here for me
attribute_database_name = "tbl_frane",
# the name without extension
dictionary_database_name = "diz_frane",
shapefile = "../euracR/data/Shapefiles/IFFI10_1.shp",
# normally null only setting it here for me
# the colums we want to retrieve directly
attri = c(
"anno_min",
"anno_max",
"mese_min",
"mese_max",
"giorno_min",
"giorno_max"
),
# tables to join the description
joins = list("tbl_frane.clas_ii_liv.acqua" = c("diz_frane.diz_acqua.acqua",
"diz_frane.diz_acqua.nome_acqua"))
)
names(test)
head(test$nome_acqua)
unique(test$nome_acqua)
count(test$nome_acqua)
plyr::count(test$nome_acqua)
test = make_shapefile(
database_dir = "../euracR/data/database/",
# normally null only setting it here for me
attribute_database_name = "tbl_frane",
# the name without extension
dictionary_database_name = "diz_frane",
shapefile = "../euracR/data/Shapefiles/IFFI10_1.shp",
# normally null only setting it here for me
# the colums we want to retrieve directly
attri = c(
"anno_min",
"anno_max",
"mese_min",
"mese_max",
"giorno_min",
"giorno_max",
"acque_stagnanti"
),
# tables to join the description
joins = list("tbl_frane.clas_ii_liv.acqua" = c("diz_frane.diz_acqua.acqua",
"diz_frane.diz_acqua.nome_acqua"))
)
nrow(test)
names(test)
length(unique(test$PIFF_ID))
df_a = data.frame(a = runif(100))
head(df_a)
df_a = data.frame(a = sample(1:100))
head(df_a)
df_b = data.frame(a = sample(1:100))
head(df_b)
sample(1:4)
sample(x = 1:10, 50)
sample(x = 1:10, 50, replace = T)
df_a = data.frame(a = sample(1:10, 100, replace = T))
df_b = data.frame(a = sample(1:10, 100, replace = T))
merge(a,b)
merge(df_a, df_b)
df_a = data.frame(b = sample(1:10, 100, replace = T))
c = merge(df_a, df_b, all.x = T)
nrow(c)
res_raw = make_shapefile(
database_dir = "../euracR/data/database/",
# normally null only setting it here for me
attribute_database_name = "tbl_frane",
# the name without extension
dictionary_database_name = "diz_frane",
shapefile = "../euracR/data/Shapefiles/IFFI10_1.shp",
# normally null only setting it here for me
# the colums we want to retrieve directly
attri = c(
"anno_min",
"anno_max",
"mese_min",
"mese_max",
"giorno_min",
"giorno_max"
),
# tables to join the description
joins = list(
"tbl_frane.Geologia.litologia" = c(
"diz_frane.diz_litologie.litologia",
"diz_frane.diz_litologie.nome_litologia"
),
"tbl_frane.clas_ii_liv.movimento" = c(
"diz_frane.diz_movimenti.movimento",
"diz_frane.diz_movimenti.nome_movimento"
),
"tbl_frane.Uso_Suolo.uso_suolo" = c(
"diz_frane.diz_usi_suolo.uso_suolo",
"diz_frane.diz_usi_suolo.nome_uso_suolo"
)
)
)
# filter all samples that have at least one for each, so we have a full date
res_filtered = res_raw %>%
dplyr::filter(
!is.na(anno_min) | !is.na(anno_max),
!is.na(mese_min) | !is.na(mese_max),
!is.na(giorno_min) | !is.na(giorno_max)
) %>%
distinct(PIFF_ID, .keep_all = TRUE)
res_no_date = res_raw %>% filter(!PIFF_ID %in% res_filtered$PIFF_ID) %>%
distinct(PIFF_ID, .keep_all = TRUE)
# build a date
res_date = res_filtered %>% mutate(
year = if_else(!is.na(anno_min), true = anno_min, false = anno_max),
month = if_else(!is.na(mese_min), true = mese_min, false = mese_max),
day = if_else(!is.na(giorno_min), true = giorno_min, false = giorno_max)
) %>% mutate(date = paste(year, month, day, sep = "/")) %>%
mutate(date = as.Date(date),
year.int = year,
year.posix = as.Date(paste0(year, "-01-01")))
# check that there are no more NAs
res_date %>%  select(c(year.posix, month, day)) %>% st_drop_geometry(.) %>% purrr::map(~sum(is.na(.)))
# 1900 - now
ggplot(res_date) +
geom_histogram(aes(x = year.posix), fill = "darkred", col = "black") +
scale_x_date(date_breaks = "10 year",
labels = scales::date_format("%Y"),
limits = as.Date(c("1900-01-01", "2020-12-01")))
# check that there are no more NAs
res_date %>%  select(c(year.posix, month, day)) %>% st_drop_geometry(.) %>% purrr::map(~sum(is.na(.)))
# 1900 - now
ggplot(res_date) +
geom_histogram(aes(x = year.posix), fill = "darkred", col = "black") +
scale_x_date(date_breaks = "10 year",
labels = scales::date_format("%Y"),
limits = as.Date(c("1900-01-01", "2020-12-01")))
# 2000 -now
ggplot(res_date, aes(x = year.posix)) +
geom_bar(fill = "grey", col = "black", size=1.2) +
geom_text(aes(label=..count..),
stat='count',
nudge_y = 0.125,
vjust=-1) +
scale_x_date(date_breaks = "1 year",
labels = scales::date_format("%Y"),
limits = as.Date(c("2000-01-02", "2020-12-01"))) +
labs(title="Count of Events of all type",
x = "Year") +
theme_minimal() +
theme(
axis.text.x = element_text(angle=90)
)
# plot it
mv = res_date %>% filter(year > 1900) %>% mapview(zcol = "year.int", at = seq(1900, 2020, 20))
mv
# 1900 - now
ggplot(res_date) +
geom_histogram(aes(x = year.posix), fill = "darkred", col = "black") +
scale_x_date(date_breaks = "10 year",
labels = scales::date_format("%Y"),
limits = as.Date(c("1900-01-01", "2020-12-01")))
type_movement_date = res_date %>% st_drop_geometry(.) %>%
select(c(nome_movimento)) %>%
mutate(movement = as.factor(nome_movimento)) %>% .[[1]]
type_movement_no_date = res_no_date %>% st_drop_geometry(.) %>%
select(c(nome_movimento)) %>%
mutate(movement = as.factor(nome_movimento)) %>% .[[1]]
# combine the two
# must adapt the size and fill with na to make one plot
n = max(length(type_movement_date), length(type_movement_no_date))
length(type_movement_date) = length(type_movement_no_date) = n
df = cbind(type_movement_date, type_movement_no_date) %>% as.data.frame(.) %>%
tidyr::pivot_longer(cols=c("type_movement_date", "type_movement_no_date"), names_to = "date", values_to="movement")
cols = c("type_movement_date" = "cornflowerblue", "type_movement_no_date" ="darkgrey")
ggplot() +
geom_bar(data = df, aes(x=movement, fill=date), position="dodge") +
scale_fill_manual(values=cols, labels=c("date present", "no date")) +
theme_minimal() +
theme(axis.text.x = element_text(angle=45, vjust = 1, hjust = 1))
# filter the translational movements
# need to drop geomtry for ggplot bar chart
translational_complete_date = res_date %>% st_drop_geometry(.) %>%
select(c(nome_movimento, year.posix)) %>%
filter(grepl(".*traslativo.*", nome_movimento))
# plot them
ggplot(translational_complete_date, aes(x=year.posix)) +
geom_bar(fill = "grey", col = "black", size=1.2) +
geom_text(aes(label=..count..),
stat='count',
nudge_y = 0.125,
vjust=-1) +
scale_x_date(date_breaks = "1 year",
labels = scales::date_format("%Y"),
limits = as.Date(c("2000-01-02", "2020-12-01"))) +
labs(title="Count of translational movements",
x = "Year",
y = "Count") +
theme_minimal() +
theme(
axis.text.x = element_text(angle=90),
axis.title.x = element_text(vjust = -1),
panel.grid.major = element_blank()
)
# plot them
ggplot(translational_complete_date, aes(x=year.posix)) +
geom_bar(fill = "grey", col = "black", size=1.2) +
geom_text(aes(label=..count..),
stat='count',
nudge_y = 0.125,
vjust=-1) +
scale_x_date(date_breaks = "1 year",
labels = scales::date_format("%Y"),
limits = as.Date(c("2000-01-02", "2020-12-01"))) +
labs(title="Count of translational movements",
x = "Year",
y = "Count") +
theme_minimal() +
theme(
axis.text.x = element_text(angle=90),
axis.title.x = element_text(vjust = -1),
panel.grid.major = element_blank()
)
nrow(translational_complete_date)
# plot them
ggplot(translational_complete_date, aes(x=year.posix)) +
geom_bar(fill = "grey", col = "black", size=1.2) +
geom_text(aes(label=..count..),
stat='count',
nudge_y = 0.125,
vjust=-1) +
scale_x_date(date_breaks = "1 year",
labels = scales::date_format("%Y"),
limits = as.Date(c("2000-01-02", "2020-12-01"))) +
labs(title="Count of translational movements",
x = "Year",
y = "Count") +
theme_minimal() +
theme(
axis.text.x = element_text(angle=90),
axis.title.x = element_text(vjust = -1),
panel.grid.major = element_blank()
)
# map of all translational movements
df_sf_transaltional_date = res_date %>%
select(c(nome_movimento, year, date)) %>%
filter(grepl("traslativo", nome_movimento))
nrow(df_sf_transaltional_date)
# only the translational
mapview_translational_with_date = mapview(df_sf_transaltional_date)
mapview_translational_with_date
# a little more interactive
df_movimento_year = res_date %>%
select(c(nome_movimento, year, date)) %>%
filter(grepl("traslativo|rotazionale", nome_movimento)) %>%
mutate(type_movement = as.factor(nome_movimento))
mapview(df_movimento_year, zcol="type_movement", burst=TRUE)
# make a map with administrative boundaries
library(GADMTools)
italy = gadm_sf_loadCountries("ITA", level=3)$sf %>% st_transform(st_crs(res_date))
south_tyrol = italy %>% filter(grepl("Bolzano", NAME_2))
#
translational_per_NAME_3 = st_intersection(df_sf_transaltional_date, south_tyrol)
count_slides = count(as_tibble(translational_per_NAME_3), "NAME_3")
# merge the numers back to the polygon
df_sf = merge(south_tyrol, count_slides, by="NAME_3", all.x=T)
## ggplot
ggplot(df_sf) +
geom_sf(aes(fill=freq)) +
scale_fill_continuous(low="white", high="red", name="count of translational slides\nwith day-precise datum", na.value="grey")
shape
shape = st_read("inst/extdata/IFFI10_1.shp")
nrow(shape)
names(shape)
length(unique(shape$PIFF_ID))
poly = st_read("../euracR/data/Shapefiles/IFFI10_5.shp")
nrow(poly)
names(poly)
length(unique(poly$PIFF_ID))
akt = st_read("../euracR/data/Shapefiles/LPM_Akten.shp")
names(akt)
head(akt$NUMEROGIS)
